{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDw7PLvLS74BMWgVbr56OB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhawana874/LLM-based-Data-Documentation-Generator/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas numpy python-dotenv gradio transformers accelerate sentencepiece\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cZNxT0d2TVYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 1) Imports & Utilities\n",
        "# =========================\n",
        "import os, re, json, textwrap, hashlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# Optional: richer profiling summary (we'll cherry-pick a few safe stats)\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "# LLM (local fallback)\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# UI\n",
        "import gradio as gr\n",
        "\n",
        "# For Colab file dialogs\n",
        "try:\n",
        "    from google.colab import files  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "print(\"Colab detected:\", IN_COLAB)\n"
      ],
      "metadata": {
        "id": "RULTv-XLUbVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 3) Lightweight PII Detection Helpers\n",
        "# =========================\n",
        "\n",
        "EMAIL_RE   = re.compile(r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\")\n",
        "PHONE_RE   = re.compile(r\"\\b(\\+?\\d{1,3}[-.\\s]?)?\\d{10}\\b\")\n",
        "AADHAAR_RE = re.compile(r\"\\b\\d{4}\\s?\\d{4}\\s?\\d{4}\\b\")         # simple heuristic\n",
        "PAN_RE     = re.compile(r\"\\b[A-Z]{5}\\d{4}[A-Z]\\b\")\n",
        "IP_RE      = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")\n",
        "\n",
        "def detect_pii_series(s: pd.Series, sample_n: int = 200) -> Dict[str, bool]:\n",
        "    \"\"\"Heuristic PII scan on a sample of values in a column.\"\"\"\n",
        "    flags = {\"email\": False, \"phone\": False, \"aadhaar\": False, \"pan\": False, \"ip\": False}\n",
        "    if s.dtype == object:\n",
        "        sample_vals = s.dropna().astype(str).head(sample_n)\n",
        "        text = \"\\n\".join(sample_vals)\n",
        "        flags[\"email\"] = bool(EMAIL_RE.search(text))\n",
        "        flags[\"phone\"] = bool(PHONE_RE.search(text))\n",
        "        flags[\"aadhaar\"] = bool(AADHAAR_RE.search(text))\n",
        "        flags[\"pan\"]    = bool(PAN_RE.search(text))\n",
        "        flags[\"ip\"]     = bool(IP_RE.search(text))\n",
        "    return flags\n",
        "\n",
        "def dtype_human(d: Any) -> str:\n",
        "    if pd.api.types.is_integer_dtype(d): return \"integer\"\n",
        "    if pd.api.types.is_float_dtype(d):   return \"float\"\n",
        "    if pd.api.types.is_bool_dtype(d):    return \"boolean\"\n",
        "    if pd.api.types.is_datetime64_any_dtype(d): return \"datetime\"\n",
        "    return \"string\"\n"
      ],
      "metadata": {
        "id": "KtH6oEbQUkVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 4) Dataset Loaders\n",
        "# =========================\n",
        "\n",
        "def load_csv_interactive() -> pd.DataFrame:\n",
        "    if not IN_COLAB:\n",
        "        raise RuntimeError(\"Interactive upload only works in Colab. Set IN_COLAB=True manually if needed.\")\n",
        "    print(\"Choose a CSV file...\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        raise RuntimeError(\"No file uploaded\")\n",
        "    fname = list(uploaded.keys())[0]\n",
        "    df = pd.read_csv(fname)\n",
        "    print(\"Loaded:\", fname, \"shape:\", df.shape)\n",
        "    return df\n",
        "\n",
        "def load_csv_path(path: str) -> pd.DataFrame:\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "# You can extend this to DBs (Postgres, Snowflake) via SQLAlchemy if needed.\n"
      ],
      "metadata": {
        "id": "mWYIYp2HUvu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 5) Schema & Quality Summary\n",
        "# =========================\n",
        "\n",
        "def summarize_dataframe(df: pd.DataFrame, sample_rows: int = 5) -> Dict[str, Any]:\n",
        "    summary = {\n",
        "        \"n_rows\": int(df.shape[0]),\n",
        "        \"n_cols\": int(df.shape[1]),\n",
        "        \"columns\": []\n",
        "    }\n",
        "    for col in df.columns:\n",
        "        s = df[col]\n",
        "        info = {\n",
        "            \"name\": col,\n",
        "            \"dtype\": dtype_human(s.dtype),\n",
        "            \"null_pct\": float(s.isna().mean() * 100),\n",
        "            \"unique_pct\": float(s.nunique(dropna=True) / max(len(s),1) * 100),\n",
        "            \"example_values\": [str(v) for v in s.dropna().unique()[:sample_rows]],\n",
        "            \"pii_flags\": detect_pii_series(s),\n",
        "        }\n",
        "        # basic stats for numeric\n",
        "        if pd.api.types.is_numeric_dtype(s):\n",
        "            desc = s.describe()\n",
        "            info[\"min\"] = float(desc.get(\"min\", np.nan)) if \"min\" in desc else None\n",
        "            info[\"max\"] = float(desc.get(\"max\", np.nan)) if \"max\" in desc else None\n",
        "            info[\"mean\"] = float(desc.get(\"mean\", np.nan)) if \"mean\" in desc else None\n",
        "        summary[\"columns\"].append(info)\n",
        "    return summary\n",
        "\n",
        "def generate_profile_html(df: pd.DataFrame, out_html=\"profile_report.html\"):\n",
        "    # optional heavy report for your own review\n",
        "    profile = ProfileReport(df, title=\"Data Profile\", minimal=True)\n",
        "    profile.to_file(out_html)\n",
        "    return out_html\n"
      ],
      "metadata": {
        "id": "WeJhrznwU3g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 6) LLM Wrappers\n",
        "# =========================\n",
        "\n",
        "SYSTEM_INSTRUCTIONS = \"\"\"You are an expert data documentation assistant.\n",
        "You produce concise, crystal-clear README documentation for tabular datasets used by data teams.\n",
        "Use plain language, avoid hype, and include concrete details from the provided schema summary.\"\"\"\n",
        "\n",
        "def prompt_for_dataset_docs(dataset_name: str, summary: Dict[str, Any]) -> str:\n",
        "    \"\"\"Compose a prompt for the LLM.\"\"\"\n",
        "    schema_json = json.dumps(summary, indent=2)\n",
        "    user_prompt = f\"\"\"\n",
        "Dataset name: {dataset_name}\n",
        "\n",
        "You are given a JSON schema/quality summary of a tabular dataset.\n",
        "Write a professional README-style documentation with these sections:\n",
        "\n",
        "1) Overview (what the dataset represents; typical use cases)\n",
        "2) Data Fields (field-by-field bullet points: meaning, type, units if any, common ranges, PII flags if detected)\n",
        "3) Data Quality (null %, uniqueness, basic issues like outliers or schema drift risk)\n",
        "4) Example Queries / Usage (2-3 practical examples in SQL or pandas)\n",
        "5) Compliance & Privacy (call out columns that may contain PII and suggest masking/anonymization)\n",
        "6) Change Management (how to track versioning/lineage; tips for stable downstream use)\n",
        "\n",
        "Constraints:\n",
        "- Be accurate and non-speculative; if unknown, say 'not specified'.\n",
        "- Keep it under ~600 words.\n",
        "- Keep a crisp, friendly tone for data engineers/analysts.\n",
        "\n",
        "Schema summary JSON:\n",
        "{schema_json}\n",
        "\"\"\"\n",
        "    return user_prompt.strip()\n",
        "\n",
        "def run_llm(prompt: str) -> str:\n",
        "    global USE_OPENAI\n",
        "    if USE_OPENAI:\n",
        "        resp = openai_client.chat.completions.create(\n",
        "            model=OPENAI_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_INSTRUCTIONS},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.2,\n",
        "        )\n",
        "        return resp.choices[0].message.content.strip()\n",
        "    else:\n",
        "        # Local FLAN-T5 fallback\n",
        "        max_in = 1024\n",
        "        tokens = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_in)\n",
        "        output = model.generate(\n",
        "            **tokens,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.2,\n",
        "            do_sample=False\n",
        "        )\n",
        "        text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        return text.strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "vwzGm4F3U73e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 7) Markdown Renderer & Saver\n",
        "# =========================\n",
        "\n",
        "def render_markdown(dataset_name: str, body: str) -> str:\n",
        "    header = f\"# {dataset_name} — Dataset Documentation\\n\\n_Last generated: {datetime.utcnow().isoformat()}Z_\\n\\n\"\n",
        "    return header + body + \"\\n\"\n",
        "\n",
        "def save_markdown(md_text: str, fname: str = \"README.md\") -> str:\n",
        "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(md_text)\n",
        "    print(\"Saved:\", fname)\n",
        "    return fname\n"
      ],
      "metadata": {
        "id": "FB7HK0AqU_Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76e5dca0"
      },
      "source": [
        "# =========================\n",
        "# Configuration\n",
        "# =========================\n",
        "\n",
        "# Set to True to use OpenAI (requires OPENAI_API_KEY set in Colab secrets)\n",
        "USE_OPENAI = False\n",
        "\n",
        "# If using OpenAI, specify the model\n",
        "OPENAI_MODEL = \"gpt-4o-mini\" # or gpt-3.5-turbo, etc.\n",
        "\n",
        "# If using local LLM, load the model and tokenizer\n",
        "if not USE_OPENAI:\n",
        "    print(\"Loading local LLM (FLAN-T5)...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "        print(\"Local LLM loaded.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading local LLM: {e}\")\n",
        "        print(\"Please ensure you have enough memory (consider a high-RAM Colab instance).\")\n",
        "        # Fallback or error handling might be needed here\n",
        "        tokenizer = None\n",
        "        model = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 8) End-to-End: Upload → Profile → Docs\n",
        "# =========================\n",
        "\n",
        "# A. Try a sample dataset if you don't have one\n",
        "sample_csv = \"\"\"customer_id,signup_date,churn_flag,email,plan,monthly_fee\n",
        "C001,2023-01-05,0,alice@example.com,Basic,9.99\n",
        "C002,2023-02-10,1,bob@example.com,Pro,19.99\n",
        "C003,2023-02-12,0,charlie@example.com,Basic,9.99\n",
        "C004,2023-03-01,0,diana@example.com,Enterprise,49.00\n",
        "C005,2023-03-15,1,ed@example.com,Pro,19.99\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_csv(StringIO(sample_csv))\n",
        "print(\"Sample df shape:\", df.shape)\n",
        "display(df.head())\n",
        "\n",
        "# B. Replace with your own upload if you want\n",
        "# df = load_csv_interactive()\n",
        "\n",
        "dataset_name = \"Customer_Subscriptions\"\n",
        "summary = summarize_dataframe(df)\n",
        "doc_prompt = prompt_for_dataset_docs(dataset_name, summary)\n",
        "doc_text = run_llm(doc_prompt)\n",
        "md = render_markdown(dataset_name, doc_text)\n",
        "print(md[:800], \"...\\n\")  # preview\n",
        "\n",
        "save_markdown(md, \"README.md\")\n",
        "\n",
        "# Optional: full profile (HTML) for your own review\n",
        "# generate_profile_html(df, \"profile_report.html\")\n"
      ],
      "metadata": {
        "id": "eZt5FUtIWeda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 9) Gradio App: Upload → Auto Doc\n",
        "# =========================\n",
        "\n",
        "def generate_docs_from_csv(file_obj) -> str:\n",
        "    try:\n",
        "        df = pd.read_csv(file_obj.name)\n",
        "    except Exception:\n",
        "        # Fallback: some browsers give bytes-like object\n",
        "        file_obj.seek(0)\n",
        "        df = pd.read_csv(file_obj)\n",
        "    dataset_name = os.path.splitext(os.path.basename(getattr(file_obj, \"name\", \"Uploaded_Dataset\")))[0]\n",
        "    summary = summarize_dataframe(df)\n",
        "    prompt = prompt_for_dataset_docs(dataset_name, summary)\n",
        "    body = run_llm(prompt)\n",
        "    md = render_markdown(dataset_name, body)\n",
        "    # Persist\n",
        "    digest = hashlib.sha256(md.encode(\"utf-8\")).hexdigest()[:8]\n",
        "    fname = f\"README_{dataset_name}_{digest}.md\"\n",
        "    save_markdown(md, fname)\n",
        "    return md\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# LLM-based Data Documentation Generator\")\n",
        "    with gr.Row():\n",
        "        file = gr.File(label=\"Upload CSV\")\n",
        "    btn = gr.Button(\"Generate Documentation\")\n",
        "    out = gr.Markdown(label=\"README Preview\")\n",
        "    btn.click(fn=generate_docs_from_csv, inputs=file, outputs=out)\n",
        "\n",
        "demo.launch(debug=False)\n"
      ],
      "metadata": {
        "id": "rVdE6XenWroY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}